{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "The goal of this practice is to perform transefer leaning on the CIFAR-10 data set with a pretrained network GoogLeNet and some quick neural training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 0. Preprations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 0.1 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 0.2 Tensorflow configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sess = None\n",
    "def reset_tf():\n",
    "    global sess\n",
    "    if sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "\n",
    "\n",
    "def reset_vars():\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. CIFAR-10 Dataset Preparation\n",
    "\n",
    "We will be working with the _CIFAR-10_ Dataset. This dataset has been preprocessed at some levels, which worked quite well for the other questions, but not for this one. Therefore, we need perform some thansformation.\n",
    "\n",
    "**Current Dataset**:\n",
    "* train_images: (imgN=50000, height=32, width=32, channelN=3)\n",
    "* validation_images: (5000, 32, 32, 3)\n",
    "\n",
    "Format: np.float, RRB scale=(0,1)\n",
    "\n",
    "**Target Transformation**:\n",
    "* GooLeNet accommodation:\n",
    "    * Zoom the image size from 32 x 32 to 288 x 288 (zoom by 7)\n",
    "    * Transef the RRB scale from (0,1) to (0,255)\n",
    "* Memory friendly\n",
    "    * use dtype=uint8:\n",
    "    * split training_images into training sets(5 batches) and one testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _save_data(save_data, save_name):\n",
    "    with open('{}.pkl'.format(save_name), 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "\n",
    "def _load_data(load_name):\n",
    "    with open('{}.pkl'.format(load_name), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def _scale_transform(input_data):\n",
    "    \"\"\"\n",
    "        input:   np.float, RGB scale=(0,1) \n",
    "        output:  np.unit8, RGB scale=(0,255)\n",
    "    \"\"\"\n",
    "    output = list(map(lambda x: x * 255, input_data))\n",
    "    return np.array(output).astype(np.uint8)\n",
    "\n",
    "def _make_batch(input_features, input_labels=None, \n",
    "                batch_num=None, batch_size=None):\n",
    "    input_num = len(input_features)\n",
    "    \n",
    "    # prerequisitions\n",
    "    if all((batch_num, batch_size)):\n",
    "        assert input_num == batch_size * batch_num\n",
    "    elif not any((batch_num, batch_size)):\n",
    "        raise NameError('Missing batch_size and batch_num')\n",
    "    \n",
    "    # compute batch_size if batch_num is provided\n",
    "    if not batch_size:   \n",
    "        batch_size = input_num // batch_num\n",
    "\n",
    "    for start in range(0, input_num, batch_size):\n",
    "        end = min(start + batch_size, input_num)\n",
    "        if input_labels is None:\n",
    "            yield input_features[start: end]\n",
    "        else:\n",
    "            yield (input_features[start: end], \n",
    "                    input_labels[start: end])\n",
    "\n",
    "def _image_zoom(image_stack, zoom_factor):\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transform_and_save(input_features, input_labels, batch_num, save_name):\n",
    "    \"\"\"\n",
    "    1) split the training_images into 5 batches\n",
    "    2) for each batch, assign 90% of the data as training samples\n",
    "    and 10% as testing samples\n",
    "    3) combine all the testing samples together\n",
    "                        \n",
    "    \"\"\"\n",
    "    valid_features, valid_labels = [], []\n",
    "    \n",
    "    for batch_i, batch_data in \\\n",
    "        enumerate(_make_batch(input_features, input_labels, batch_num=batch_num)):\n",
    "        \n",
    "        batch_features, batch_labels = batch_data\n",
    "        \n",
    "        # transform\n",
    "        scaled_features = _scale_transform(batch_features)\n",
    "        val_idx = int(len(batch_labels) * 0.1)\n",
    "       \n",
    "        # save training - 90% data\n",
    "        _save_data((scaled_features[:-val_idx],\n",
    "                    batch_labels[:-val_idx]),\n",
    "                   '{}_{}{:02d}'.format(save_name, 'train', batch_i))\n",
    "        \n",
    "        # combing testing - 10% data\n",
    "        valid_features.extend(scaled_features[-val_idx:])\n",
    "        valid_labels.extend(batch_labels[-val_idx:])\n",
    "    \n",
    "    #save testing\n",
    "    _save_data((np.array(valid_features), np.array(valid_labels)),\n",
    "               '{}_{}'.format(save_name, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## settings\n",
    "batch_num = 5\n",
    "\n",
    "## load data \n",
    "train_images = np.load(open('train_images.npy', 'rb'))\n",
    "train_labels = np.load(open('train_labels.npy', 'rb'))\n",
    "validation_images = np.load(open('validation_images.npy', 'rb'))\n",
    "\n",
    "## train\n",
    "transform_and_save(train_images, train_labels, batch_num, 'CIFAR')\n",
    "\n",
    "# validation\n",
    "valid_data = _scale_transform(validation_images)\n",
    "_save_data(valid_data, 'CIFAR_validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIRMING SAMPLE SIZES:\n",
      "\n",
      "data     | features | labels\n",
      "============================\n",
      "train00  | 9000     | 9000 \n",
      "train01  | 9000     | 9000 \n",
      "train02  | 9000     | 9000 \n",
      "train03  | 9000     | 9000 \n",
      "train04  | 9000     | 9000 \n",
      "test     | 5000     | 5000 \n",
      "validate | 10000    |\n"
     ]
    }
   ],
   "source": [
    "batch_num = 5\n",
    "\n",
    "files = ['train{:02d}'.format(i) for i in range(batch_num)]\n",
    "files.extend(('test', 'validate'))\n",
    "\n",
    "print('CONFIRMING SAMPLE SIZES:\\n')\n",
    "print('{0: <8} | {1: <8} | {2: <5}'.format('data', 'features', 'labels'))\n",
    "print('{}'.format('=' * 28))\n",
    "for file in files:\n",
    "    data = _load_data('CIFAR_{}'.format(file))\n",
    "    if file == 'validate':\n",
    "        print('{0: <8} | {1: <8} |'.format(file, data.shape[0]))\n",
    "    else:\n",
    "        print('{0: <8} | {1: <8} | {2: <5}'.format(file, data[0].shape[0], data[1].shape[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the output of the pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\"\n",
    "data_dir = os.path.expanduser(\"~/inception/5h/\")\n",
    "file_path = os.path.join(data_dir, 'inception5h.zip')\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    # Check if the download directory exists, otherwise create it.\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    # Download\n",
    "    with open(file_path, \"wb\") as local_file:\n",
    "        local_file.write(urlopen(data_url).read())\n",
    "    # Extract\n",
    "    zipfile.ZipFile(file_path, mode=\"r\").extractall(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(data_dir, \"tensorflow_inception_graph.pb\")\n",
    "with tf.gfile.FastGFile(path, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    gnet_input = sess.graph.get_tensor_by_name(\"input:0\")\n",
    "    gnet_output = tf.squeeze(sess.graph.get_tensor_by_name(\"avgpool0:0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate output\n",
    "becuase of the large sample sizes, we need to batch this calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_gnet_output(input_features, batch_size):\n",
    "    output = []\n",
    "    start_time = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        gnet_input = sess.graph.get_tensor_by_name(\"input:0\")\n",
    "        gnet_output = tf.squeeze(sess.graph.get_tensor_by_name(\"avgpool0:0\"))\n",
    "\n",
    "        batched_data = _make_batch(input_features, batch_size=batch_size)\n",
    "        for i, batch in enumerate(batched_data):\n",
    "            past_time = time.time() - start_time\n",
    "            print(i, end=' ')\n",
    "            zoomed = zoom(batch, (1, 7, 7, 1), order = 1)\n",
    "            output.extend(sess.run(gnet_output, \n",
    "                          {gnet_input: zoomed}))\n",
    "    return np.array(output)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent(files):\n",
    "    \"\"\"\n",
    "    get the latent output of GoogLeNet and save\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        load_name = 'CIFAR_{}'.format(file) \n",
    "        load = _load_data(load_name)[0]\n",
    "        output = _get_gnet_output(load, batch_size=50)\n",
    "        save_name = 'latent_{}'.format(file)\n",
    "        _save_data(output, save_name)\n",
    "        print(' ')\n",
    "\n",
    "# \n",
    "get_latent(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check latent output size\n",
      "train00  (9000, 1024)\n",
      "train01  (9000, 1024)\n",
      "train02  (9000, 1024)\n",
      "train03  (9000, 1024)\n",
      "train04  (9000, 1024)\n",
      "test     (5000, 1024)\n",
      "validate (10000, 1024)\n"
     ]
    }
   ],
   "source": [
    "print('check latent output size')\n",
    "for file in files:\n",
    "    load = _load_data('latent_{}'.format(file))\n",
    "    print('{0: <8} {1}'.format(file, load.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neuralnet Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1024], name=\"latent\")\n",
    "y_label = tf.placeholder(tf.int64, [None,], name=\"labels\")\n",
    "training = tf.placeholder(tf.bool, name=\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00001\n",
    "EPOCH_NUM = 50\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.2\n",
    "HIDDEN_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Full-connected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop1 = tf.layers.dropout(x, DROPOUT, training=training)\n",
    "hidden1 = tf.layers.dense(drop1, HIDDEN_SIZE , activation=tf.nn.relu, use_bias=True,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=LATENT_N ** -0.5))\n",
    "\n",
    "drop2 = tf.layers.dropout(hidden1, DROPOUT, training=training)\n",
    "hidden2 = tf.layers.dense(drop2, HIDDEN_SIZE , activation=tf.nn.relu, use_bias=True,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=HIDDEN_SIZE **-0.5))\n",
    "\n",
    "drop3 = tf.layers.dropout(hidden2, DROPOUT, training=training)\n",
    "hidden3 = tf.layers.dense(drop3, HIDDEN_SIZE , activation=tf.nn.relu, use_bias=True,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=HIDDEN_SIZE **-0.5))\n",
    "\n",
    "drop4 = tf.layers.dropout(hidden3, DROPOUT, training=training)\n",
    "y = tf.layers.dense(drop4, 10, activation=None, use_bias=True,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=HIDDEN_SIZE **-0.5))\n",
    "\n",
    "\n",
    "## Define loss, train, and accuracy tensor/Operation\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "loss = tf.reduce_mean(softmax(logits=y,\n",
    "                              labels=tf.one_hot(y_label, 10)))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), y_label), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "raw_X_train, raw_y_train = [], []\n",
    "for i in range(batch_num):\n",
    "    raw_X_train.extend(_load_data('latent_train{:02d}'.format(i)))\n",
    "    raw_y_train.extend(_load_data('CIFAR_train{:02d}'.format(i))[1])\n",
    "X_train = np.array(raw_X_train)\n",
    "y_train = np.array(raw_y_train)\n",
    "\n",
    "# testing data\n",
    "X_test = _load_data('latent_test')\n",
    "_, y_test = _load_data('CIFAR_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "EPOCH0: test acc=0.811200\n",
      "EPOCH1: test acc=0.826200\n",
      "EPOCH2: test acc=0.825600\n",
      "EPOCH3: test acc=0.842800\n",
      "EPOCH4: test acc=0.839600\n",
      "EPOCH5: test acc=0.856800\n",
      "EPOCH6: test acc=0.853400\n",
      "EPOCH7: test acc=0.851400\n",
      "EPOCH8: test acc=0.856200\n",
      "EPOCH9: test acc=0.855600\n",
      "EPOCH10: test acc=0.864200\n",
      "EPOCH11: test acc=0.846400\n",
      "EPOCH12: test acc=0.861000\n",
      "EPOCH13: test acc=0.856600\n",
      "EPOCH14: test acc=0.854400\n",
      "EPOCH15: test acc=0.865200\n",
      "EPOCH16: test acc=0.861000\n",
      "EPOCH17: test acc=0.855600\n",
      "EPOCH18: test acc=0.857200\n",
      "EPOCH19: test acc=0.864000\n",
      "EPOCH20: test acc=0.867000\n",
      "EPOCH21: test acc=0.854800\n",
      "EPOCH22: test acc=0.860800\n",
      "EPOCH23: test acc=0.866600\n",
      "EPOCH24: test acc=0.866000\n",
      "EPOCH25: test acc=0.863600\n",
      "EPOCH26: test acc=0.861600\n",
      "EPOCH27: test acc=0.855200\n",
      "EPOCH28: test acc=0.864800\n",
      "EPOCH29: test acc=0.864800\n",
      "EPOCH30: test acc=0.864600\n",
      "EPOCH31: test acc=0.869800\n",
      "EPOCH32: test acc=0.866800\n",
      "EPOCH33: test acc=0.867400\n",
      "EPOCH34: test acc=0.860000\n",
      "EPOCH35: test acc=0.861000\n",
      "EPOCH36: test acc=0.863600\n",
      "EPOCH37: test acc=0.866800\n",
      "EPOCH38: test acc=0.867600\n",
      "EPOCH39: test acc=0.869800\n",
      "EPOCH40: test acc=0.862000\n",
      "EPOCH41: test acc=0.865800\n",
      "EPOCH42: test acc=0.869200\n",
      "EPOCH43: test acc=0.868200\n",
      "EPOCH44: test acc=0.870000\n",
      "EPOCH45: test acc=0.867600\n",
      "EPOCH46: test acc=0.861600\n",
      "EPOCH47: test acc=0.870000\n",
      "EPOCH48: test acc=0.860800\n",
      "EPOCH49: test acc=0.862800\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "# Initializing the variables\n",
    "reset_vars()\n",
    "\n",
    "for epoch_i in range(EPOCH_NUM):\n",
    "    train_acc, test_acc = [], []\n",
    "\n",
    "    j = np.random.choice(len(y_train), len(y_train) // 2, replace=False)\n",
    "    X_train_select =  X_train[j,:]\n",
    "    y_train_select =  y_train[j].reshape(-1,)\n",
    "\n",
    "    # training\n",
    "    train_batches = _make_batch(X_train_select, y_train_select, batch_size = BATCH_SIZE)\n",
    "    for X_train_batch, y_train_batch in train_batches: \n",
    "        opt_dict = {x: X_train_batch, y_label: y_train_batch, training: True}\n",
    "        sess.run(optimizer, feed_dict=opt_dict)\n",
    "\n",
    "    # cross_validation accuracy\n",
    "    acc_dict = {x: X_test, y_label: y_test, training:False}\n",
    "    test_acc = sess.run(accuracy, feed_dict=acc_dict)\n",
    "\n",
    "    # acc_dict = {x: X_train, y_label: y_train, training:False}\n",
    "    # train_acc = sess.run(accuracy, feed_dict=acc_dict)\n",
    "\n",
    "    # output \n",
    "    print('EPOCH{}: test acc={:05f}'.format(epoch_i, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate = _load_data('latent_validate')\n",
    "pred = sess.run(y, feed_dict={x: X_validate, training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.937601  , -24.62299   ,   1.5771306 , ...,  18.127853  ,\n",
       "        -29.210209  , -27.568802  ],\n",
       "       [ -6.0771875 ,   9.587309  , -12.616006  , ..., -14.007324  ,\n",
       "         -1.281367  ,  15.077904  ],\n",
       "       [  0.45665035,  -2.5449746 ,  -1.8046563 , ...,  -1.3174105 ,\n",
       "         -0.16612701,  -2.9723592 ],\n",
       "       ...,\n",
       "       [ -7.7910743 ,   6.1794963 , -12.122572  , ..., -10.409006  ,\n",
       "         -0.57737803,  17.197119  ],\n",
       "       [ -6.2540145 , -12.724137  ,   5.22315   , ...,  -0.08172381,\n",
       "         -8.927856  , -12.070505  ],\n",
       "       [-10.90924   ,  -9.936182  ,  -4.9890885 , ...,   2.2050624 ,\n",
       "        -10.702816  , -11.3396635 ]], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
